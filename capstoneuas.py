# -*- coding: utf-8 -*-
"""CapstoneUAS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EGdkRPA1om0pFTIbiPfMxGqVKMqYRl7B
"""

# Mengimpor library pandas
import pandas as pd

# Membaca dataset ObesityDataSet.csv
df = pd.read_csv('D:/Documents/Downloads/ObesityDataSet.csv')

# Menampilkan 5 baris pertama dataset
df.head()

# Menampilkan jumlah baris dan kolom dalam dataset
print("Jumlah Baris dan Kolom:", df.shape)

# Menampilkan ringkasan informasi dataset (jumlah non-null, tipe data)
df.info()

# Menampilkan tipe data masing-masing kolom
df.dtypes

# Menampilkan statistik deskriptif semua kolom (termasuk kategorikal)
df.describe(include='all')

# Membuat salinan data yang telah dibersihkan dari duplikat
df_cleaned = df.copy()

# Cek dan hapus duplikat
df_cleaned = df_cleaned.drop_duplicates()

# Cek missing values
df_cleaned.isnull().sum()

# Konversi kolom numerik ke float
for col in ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']:
    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')

# Tangani missing values: numerik → median, kategorikal → modus
# Kolom numerik
num_cols = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']
for col in num_cols:
    df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)

# Kolom kategorikal/biner
cat_cols = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']
for col in cat_cols:
    df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)

# Pastikan sudah tidak ada missing values
df_cleaned.isnull().sum()

# Konversi kolom numerik yang masih bertipe object
cols_to_convert = ['Age', 'Height', 'Weight', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']
for col in cols_to_convert:
    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')  # jika ada error parse, akan jadi NaN

# Periksa kembali tipe data
print(df_cleaned.dtypes)

# Deteksi data outlier
from scipy.stats import zscore
import numpy as np

z_scores = np.abs(zscore(df_cleaned[num_cols]))
outliers = (z_scores > 3).any(axis=1)

print(f"Jumlah data outlier: {outliers.sum()}")

# Standarisasi data numerik
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_cleaned[num_cols] = scaler.fit_transform(df_cleaned[num_cols])

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.countplot(x='NObeyesdad', data=df_cleaned)
plt.title('Distribusi Kelas Target (Obesity Level)')
plt.xticks(rotation=45)
plt.show()

# Memisahkan fitur dan target
X = df_cleaned.drop('NObeyesdad', axis=1)
y = df_cleaned['NObeyesdad']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# 1. Pisahkan kolom numerik dan kategori
numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X_train.select_dtypes(include=['object']).columns

# 2. Imputasi
num_imputer = SimpleImputer(strategy='median')
X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[numerical_cols]), columns=numerical_cols)
X_test_num = pd.DataFrame(num_imputer.transform(X_test[numerical_cols]), columns=numerical_cols)

cat_imputer = SimpleImputer(strategy='most_frequent')
X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train[categorical_cols]), columns=categorical_cols)
X_test_cat = pd.DataFrame(cat_imputer.transform(X_test[categorical_cols]), columns=categorical_cols)

# 3. Onehot encoding kategori
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoder.fit(X_train_cat)

X_train_cat_encoded = pd.DataFrame(encoder.transform(X_train_cat), columns=encoder.get_feature_names_out(categorical_cols))
X_test_cat_encoded = pd.DataFrame(encoder.transform(X_test_cat), columns=encoder.get_feature_names_out(categorical_cols))

# 4. Gabungkan kembali
X_train_final = pd.concat([X_train_num.reset_index(drop=True), X_train_cat_encoded.reset_index(drop=True)], axis=1)
X_test_final = pd.concat([X_test_num.reset_index(drop=True), X_test_cat_encoded.reset_index(drop=True)], axis=1)

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

rf = RandomForestClassifier(random_state=42)
knn = KNeighborsClassifier()
svc = SVC()

rf.fit(X_train_final, y_train)
knn.fit(X_train_final, y_train)
svc.fit(X_train_final, y_train)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Prediksi model
y_pred_rf = rf.predict(X_test_final)
y_pred_knn = knn.predict(X_test_final)
y_pred_svc = svc.predict(X_test_final)

# Evaluasi fungsi
def evaluate_model(y_true, y_pred):
    return {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, average='weighted', zero_division=0),
        "Recall": recall_score(y_true, y_pred, average='weighted', zero_division=0),
        "F1 Score": f1_score(y_true, y_pred, average='weighted', zero_division=0)
    }

# Evaluasi setiap model
eval_rf = evaluate_model(y_test, y_pred_rf)
eval_knn = evaluate_model(y_test, y_pred_knn)
eval_svc = evaluate_model(y_test, y_pred_svc)

# Buat dataframe hasil evaluasi
results_df = pd.DataFrame({
    'RandomForest': eval_rf,
    'KNN': eval_knn,
    'SVC': eval_svc
})

# Transpose agar model jadi sumbu X
results_df = results_df.T

# Plot heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(results_df, annot=True, cmap='Blues', fmt=".2f")
plt.title("Evaluasi Model (Accuracy, Precision, Recall, F1 Score)")
plt.xlabel("Metric")
plt.ylabel("Model")
plt.tight_layout()
plt.show()

# Bar chart sebagai pelengkap setelah heatmap
plt.figure(figsize=(10, 6))

# Plot bar chart dari dataframe yang telah ditranspose (results_df sudah hasil evaluasi)
results_df.plot(kind='bar', figsize=(10, 6))

plt.title("Perbandingan Performa Antar Model")
plt.ylabel("Score")
plt.xlabel("Model")
plt.xticks(rotation=0)
plt.ylim(0, 1.05)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(title='Metric')
plt.tight_layout()
plt.show()

# Hitung korelasi hanya untuk fitur numerik
correlation_matrix = X_train_num.corr()

# Tampilkan heatmap korelasi
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, linewidths=0.5)

plt.title("Heatmap Korelasi Fitur Numerik")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV

# Parameter grid untuk setiap model
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

param_grid_knn = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance']
}

param_grid_svc = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Inisialisasi model
rf = RandomForestClassifier(random_state=42)
knn = KNeighborsClassifier()
svc = SVC(probability=True)

# GridSearchCV
grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)
grid_svc = GridSearchCV(svc, param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)

# Latih model
grid_rf.fit(X_train_final, y_train)
grid_knn.fit(X_train_final, y_train)
grid_svc.fit(X_train_final, y_train)

# Simpan model terbaik
best_rf = grid_rf.best_estimator_
best_knn = grid_knn.best_estimator_
best_svc = grid_svc.best_estimator_

# Print parameter terbaik
print("Best parameters RF:", grid_rf.best_params_)
print("Best parameters KNN:", grid_knn.best_params_)
print("Best parameters SVC:", grid_svc.best_params_)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
# Hypertuning untuk random forest
# Parameter grid
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

# Inisialisasi dan GridSearchCV
rf = RandomForestClassifier(random_state=42)
grid_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)
grid_rf.fit(X_train_final, y_train)

# Simpan model terbaik
best_rf = grid_rf.best_estimator_

# Cetak parameter terbaik
print("Best parameters RF:", grid_rf.best_params_)

from sklearn.neighbors import KNeighborsClassifier
# Hypertuning untuk KNN
param_grid_knn = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance']
}

knn = KNeighborsClassifier()
grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)
grid_knn.fit(X_train_final, y_train)

best_knn = grid_knn.best_estimator_
print("Best parameters KNN:", grid_knn.best_params_)

from sklearn.svm import SVC
# Hypertuning untuk SVC
param_grid_svc = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

svc = SVC(probability=True)
grid_svc = GridSearchCV(svc, param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)
grid_svc.fit(X_train_final, y_train)

best_svc = grid_svc.best_estimator_
print("Best parameters SVC:", grid_svc.best_params_)

# Prediksi menggunakan model terbaik
y_pred_best_rf = best_rf.predict(X_test_final)
y_pred_best_knn = best_knn.predict(X_test_final)
y_pred_best_svc = best_svc.predict(X_test_final)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Evaluasi performa model terbaik
def evaluate_model(y_true, y_pred):
    return {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, average='weighted', zero_division=0),
        "Recall": recall_score(y_true, y_pred, average='weighted', zero_division=0),
        "F1 Score": f1_score(y_true, y_pred, average='weighted', zero_division=0)
    }

eval_best_rf = evaluate_model(y_test, y_pred_best_rf)
eval_best_knn = evaluate_model(y_test, y_pred_best_knn)
eval_best_svc = evaluate_model(y_test, y_pred_best_svc)

# Membuat dataframe untuk hasil evaluasi setelah tuning
tuned_results_df = pd.DataFrame({
    'RandomForest (Tuned)': eval_best_rf,
    'KNN (Tuned)': eval_best_knn,
    'SVC (Tuned)': eval_best_svc
}).T

# Menampilkan heatmap hasil setelah tuning
plt.figure(figsize=(10, 6))
sns.heatmap(tuned_results_df, annot=True, cmap='Greens', fmt=".2f")
plt.title("Evaluasi Model Setelah Hypertuning")
plt.xlabel("Metric")
plt.ylabel("Model")
plt.tight_layout()
plt.show()

# Gabungkan kedua hasil
combined_results = pd.concat([results_df, tuned_results_df])

# Plot heatmap perbandingan sebelum dan setelah tuning
plt.figure(figsize=(12, 6))
sns.heatmap(combined_results, annot=True, cmap='YlGnBu', fmt=".2f")
plt.title("Perbandingan Performa Model: Sebelum vs Sesudah Hypertuning")
plt.xlabel("Metric")
plt.ylabel("Model")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Setelah dilakukan serangkaian eksperimen pada tiga algoritma klasifikasi yaitu Random Forest, K-Nearest Neighbors (KNN), dan Support Vector Classifier (SVC), diperoleh beberapa temuan menarik dari proses evaluasi sebelum dan sesudah dilakukan hyperparameter tuning.

Sebelum Tuning
Ketiga model sudah menunjukkan performa yang cukup baik. Nilai akurasi, precision, recall, dan F1-score semuanya berada di atas standar minimal yang diharapkan. Dari hasil heatmap dan grafik bar, terlihat bahwa Random Forest unggul sedikit di atas KNN dan SVC.

Setelah Tuning (Hyperparameter Tuning)
Setelah dilakukan proses tuning menggunakan GridSearchCV, ternyata terjadi peningkatan performa pada KNN dan SVC. Hal ini menunjukkan bahwa pemilihan parameter yang lebih optimal mampu meningkatkan kemampuan model dalam melakukan klasifikasi.

Namun, pada model Random Forest, performanya justru sedikit menurun (sekitar 0.01). Kemungkinan besar hal ini disebabkan oleh kombinasi parameter yang kurang cocok terhadap karakteristik dataset, atau bisa juga karena model default sebelumnya memang sudah cukup optimal tanpa perlu banyak penyesuaian.

Perbandingan Sebelum dan Sesudah Tuning
Dari hasil perbandingan akhir, bisa disimpulkan bahwa tuning punya pengaruh cukup signifikan terhadap model KNN dan SVC, terutama dalam meningkatkan skor evaluasi. Meskipun Random Forest mengalami sedikit penurunan, selisihnya masih tergolong kecil dan tidak terlalu berdampak besar secara keseluruhan.
"""

# model_evaluation_app.py
import streamlit as st

# --- Judul Aplikasi ---
st.title("📊 Evaluasi Model ML Sebelum & Sesudah Hyperparameter Tuning")

# --- Hasil Evaluasi Sebelum Tuning ---
results_df_before = pd.DataFrame({
    'Accuracy': [0.94, 0.79, 0.86],
    'Precision': [0.95, 0.78, 0.87],
    'Recall': [0.94, 0.79, 0.86],
    'F1 Score': [0.94, 0.78, 0.86]
}, index=['RandomForest', 'KNN', 'SVC'])

# --- Hasil Evaluasi Setelah Tuning ---
results_df_after = pd.DataFrame({
    'Accuracy': [0.93, 0.81, 0.90],
    'Precision': [0.94, 0.81, 0.90],
    'Recall': [0.93, 0.81, 0.90],
    'F1 Score': [0.93, 0.80, 0.90]
}, index=['RandomForest', 'KNN', 'SVC'])

# --- Tampilkan Data Sebelum Tuning ---
st.subheader("🔍 Hasil Evaluasi Sebelum Tuning")
st.dataframe(results_df_before.style.format("{:.2f}"))

fig1, ax1 = plt.subplots()
sns.heatmap(results_df_before, annot=True, cmap="Blues", fmt=".2f", ax=ax1)
ax1.set_title("Heatmap - Sebelum Tuning")
st.pyplot(fig1)

# --- Tampilkan Data Setelah Tuning ---
st.subheader("🛠️ Hasil Evaluasi Setelah Tuning")
st.dataframe(results_df_after.style.format("{:.2f}"))

fig2, ax2 = plt.subplots()
sns.heatmap(results_df_after, annot=True, cmap="Greens", fmt=".2f", ax=ax2)
ax2.set_title("Heatmap - Setelah Tuning")
st.pyplot(fig2)

# --- Bar Chart Perbandingan Setiap Metric ---
st.subheader("📈 Perbandingan Skor Sebelum & Sesudah Tuning")
metrics = results_df_before.columns

for metric in metrics:
    fig, ax = plt.subplots()
    bar_df = pd.DataFrame({
        'Before Tuning': results_df_before[metric],
        'After Tuning': results_df_after[metric]
    })
    bar_df.plot(kind='bar', ax=ax)
    ax.set_title(f"Perbandingan {metric}")
    ax.set_ylabel(metric)
    ax.set_ylim(0, 1.05)
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    st.pyplot(fig)

